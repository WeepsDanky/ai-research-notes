# Popular Datasets for LLMs 

Sure â€” hereâ€™s a clean **Markdown summary** of all datasets mentioned, grouped into the three main categories:

---

## ğŸ§© **A. Datasets for Basic Tasks**

*(Language modeling, understanding, generation)*

* **Natural Questions (NQ)** â€” Real Google search queries paired with Wikipedia pages; annotators mark long/short answers or null.
* **MMLU** â€” Multi-task benchmark over 57 subjects to assess zero/few-shot general knowledge and reasoning.
* **MBPP (Mostly Basic Python Problems)** â€” 974 short Python programming tasks with solutions and test cases for code generation.
* **HumanEval** â€” 164 hand-crafted programming problems with function signatures and unit tests; used to test code generation.
* **APPS** â€” 232K Python programs and 10K text-based problems with test cases; evaluates code generation.
* **WikiSQL** â€” 87K natural languageâ€“SQL query pairs from Wikipedia tables; tests text-to-SQL ability.
* **TriviaQA** â€” 650K question-answer-evidence triples authored by trivia enthusiasts, using Wikipedia and web sources.
* **RACE** â€” English reading-comprehension exams from Chinese students (middle/high school), ~100K questions.
* **SQuAD** â€” 100K questionâ€“answer pairs from Wikipedia passages; answers are text spans.
* **BoolQ** â€” Yes/no QA dataset with 16K questions and supporting paragraphs.
* **MultiRC** â€” Multi-sentence reading comprehension requiring reasoning across sentences; ~6K questions from 800 paragraphs.

---

## ğŸ§  **B. Datasets for Emergent Abilities**

*(In-context learning, reasoning, instruction following)*

* **GSM8K** â€” 8.5K grade-school math word problems requiring multi-step arithmetic reasoning.
* **MATH** â€” 12.5K high-school competition math problems with step-by-step LATEX solutions.
* **HellaSwag** â€” 70K multiple-choice commonsense reasoning questions derived from ActivityNet/WikiHow.
* **AI2 Reasoning Challenge (ARC)** â€” 7.8K science exam questions (Easy/Challenge sets) for commonsense reasoning.
* **PIQA** â€” Physical commonsense benchmark; choose between two plausible actions in daily scenarios.
* **SIQA (Social IQA)** â€” 38K multiple-choice questions testing social and emotional commonsense.
* **OpenBookQA (OBQA)** â€” 6K multiple-choice science questions requiring external commonsense facts.
* **TruthfulQA** â€” 817 questions across 38 categories (health, law, politics) testing factual truthfulness.
* **OPT-IML Bench** â€” 2K NLP tasks from 8 benchmarks (~18M examples) for instruction meta-learning evaluation.

---

## âš™ï¸ **C. Datasets for Augmented Abilities**

*(Using external knowledge, retrieval, or tools)*

* **HotpotQA** â€” 113K multi-hop reasoning questions from Wikipedia; requires combining two supporting paragraphs.
* **ToolQA** â€” QA benchmark testing an LLMâ€™s ability to call and use external tools.
* **GPT4Tools** â€” Instruction dataset for tool usage generated from multimodal teacher models; includes train/validation/test splits with ~71K examples.

